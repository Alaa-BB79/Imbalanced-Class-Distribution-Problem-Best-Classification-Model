{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve\nfrom sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom scipy.stats import entropy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ImbalancedDataAnalyzer:\n    def __init__(self):\n        self.feature_selectors = {\n            'Anova': self._anova_selection,\n            'Chi2': self._chi2_selection,\n            'InfoGain': self._infogain_selection,\n            'InfoGainRatio': self._infogain_ratio_selection,\n            'Relief': self._relief_selection,\n            'GiniDecrease': self._gini_decrease_selection,\n            'FCBF': self._fcbf_selection\n        }\n        \n        self.classifiers = {\n            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n            'DecisionTree': DecisionTreeClassifier(random_state=42),\n            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n            'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),\n            'NaiveBayes': GaussianNB(),\n            'SVM': SVC(probability=True, random_state=42)\n        }\n        \n        self.results_df = None\n        self.best_model = None\n        self.best_selector = None\n        \n    def _anova_selection(self, X, y, k):\n        selector = SelectKBest(score_func=f_classif, k=k)\n        X_selected = selector.fit_transform(X, y)\n        return X_selected, selector.get_support()\n\n    def _chi2_selection(self, X, y, k):\n        # Ensure data is non-negative for chi2\n        X_scaled = X - X.min() + 1e-6\n        selector = SelectKBest(score_func=chi2, k=k)\n        X_selected = selector.fit_transform(X_scaled, y)\n        return X_selected, selector.get_support()\n\n    def _infogain_selection(self, X, y, k):\n        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n        X_selected = selector.fit_transform(X, y)\n        return X_selected, selector.get_support()\n\n    def _infogain_ratio_selection(self, X, y, k):\n        def calculate_igr(X, y):\n            mi = mutual_info_classif(X, y)\n            intrinsic_value = np.array([entropy(pd.qcut(col, q=10, duplicates='drop').codes) \n                                      for col in X.T])\n            intrinsic_value = np.where(intrinsic_value == 0, 1e-10, intrinsic_value)\n            return mi / intrinsic_value\n\n        scores = calculate_igr(X, y)\n        selected_features = np.argsort(scores)[-k:]\n        mask = np.zeros(X.shape[1], dtype=bool)\n        mask[selected_features] = True\n        return X.iloc[:, selected_features], mask\n\n    def _relief_selection(self, X, y, k):\n        def compute_relief_scores(X, y):\n            n_samples, n_features = X.shape\n            weights = np.zeros(n_features)\n            \n            for i in range(n_samples):\n                same_class = X[y == y[i]]\n                diff_class = X[y != y[i]]\n                \n                # Find nearest hit and miss\n                nearest_hit = same_class[np.argmin(np.sum((same_class - X[i])**2, axis=1))]\n                nearest_miss = diff_class[np.argmin(np.sum((diff_class - X[i])**2, axis=1))]\n                \n                weights += np.abs(X[i] - nearest_miss) - np.abs(X[i] - nearest_hit)\n            \n            return weights / n_samples\n\n        scores = compute_relief_scores(X.values, y)\n        selected_features = np.argsort(scores)[-k:]\n        mask = np.zeros(X.shape[1], dtype=bool)\n        mask[selected_features] = True\n        return X.iloc[:, selected_features], mask\n\n    def _gini_decrease_selection(self, X, y, k):\n        forest = ExtraTreesClassifier(n_estimators=100, random_state=42)\n        forest.fit(X, y)\n        scores = forest.feature_importances_\n        selected_features = np.argsort(scores)[-k:]\n        mask = np.zeros(X.shape[1], dtype=bool)\n        mask[selected_features] = True\n        return X.iloc[:, selected_features], mask\n\n    def _fcbf_selection(self, X, y, k):\n        def symmetrical_uncertainty(x, y):\n            mutual_info = mutual_info_classif(x.reshape(-1, 1), y)[0]\n            x_entropy = entropy(pd.qcut(x, q=10, duplicates='drop').codes)\n            y_entropy = entropy(pd.qcut(y, q=10, duplicates='drop').codes)\n            if x_entropy == 0 or y_entropy == 0:\n                return 0\n            return 2.0 * mutual_info / (x_entropy + y_entropy)\n\n        # Calculate SU between features and class\n        su_scores = np.array([symmetrical_uncertainty(X.iloc[:, i], y) \n                            for i in range(X.shape[1])])\n        \n        # Select top k features based on SU scores\n        selected_features = np.argsort(su_scores)[-k:]\n        mask = np.zeros(X.shape[1], dtype=bool)\n        mask[selected_features] = True\n        return X.iloc[:, selected_features], mask\n\n    def evaluate_model(self, y_true, y_pred, y_prob):\n        return {\n            'F1_Score': f1_score(y_true, y_pred),\n            'MCC': matthews_corrcoef(y_true, y_pred),\n            'AUC': roc_auc_score(y_true, y_prob[:, 1])\n        }\n\n    def analyze_dataset(self, X, y, feature_percentages=[0.75, 0.5]):\n        results = []\n        n_features = X.shape[1]\n        \n        # Calculate imbalance ratio\n        class_counts = np.bincount(y)\n        imbalance_ratio = class_counts.max() / class_counts.min()\n        \n        print(f\"Dataset Information:\")\n        print(f\"Number of features: {n_features}\")\n        print(f\"Number of samples: {len(y)}\")\n        print(f\"Class distribution: {dict(zip(range(len(class_counts)), class_counts))}\")\n        print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n        \n        # Split and scale data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        scaler = StandardScaler()\n        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n        \n        best_f1 = 0\n        \n        for percentage in feature_percentages:\n            k = max(1, int(n_features * percentage))\n            print(f\"\\nAnalyzing with {percentage*100}% of features ({k} features)\")\n            \n            for fs_name, fs_func in self.feature_selectors.items():\n                try:\n                    print(f\"\\nApplying {fs_name} feature selection...\")\n                    X_train_selected, feature_mask = fs_func(X_train_scaled, y_train, k)\n                    X_test_selected = X_test_scaled.iloc[:, feature_mask]\n                    \n                    for clf_name, clf in self.classifiers.items():\n                        print(f\"Training {clf_name}...\", end=' ')\n                        clf.fit(X_train_selected, y_train)\n                        y_pred = clf.predict(X_test_selected)\n                        y_prob = clf.predict_proba(X_test_selected)\n                        \n                        metrics = self.evaluate_model(y_test, y_pred, y_prob)\n                        \n                        # Track best model\n                        if metrics['F1_Score'] > best_f1:\n                            best_f1 = metrics['F1_Score']\n                            self.best_model = clf\n                            self.best_selector = (fs_func, feature_mask)\n                        \n                        results.append({\n                            'Feature_Selection': fs_name,\n                            'Classifier': clf_name,\n                            'Feature_Percentage': percentage,\n                            'Imbalance_Ratio': imbalance_ratio,\n                            **metrics\n                        })\n                        print(f\"F1: {metrics['F1_Score']:.3f}, MCC: {metrics['MCC']:.3f}, AUC: {metrics['AUC']:.3f}\")\n                        \n                except Exception as e:\n                    print(f\"Error with {fs_name} and {clf_name}: {str(e)}\")\n                    continue\n        \n        self.results_df = pd.DataFrame(results)\n        return self.results_df\n\n    def plot_results(self):\n        if self.results_df is None:\n            print(\"No results available. Please run analyze_dataset first.\")\n            return\n        \n        # Create figure with subplots\n        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n        \n        # Plot 1: Feature Selection Methods Comparison\n        sns.boxplot(data=self.results_df, x='Feature_Selection', y='F1_Score', ax=axes[0,0])\n        axes[0,0].set_title('Feature Selection Methods Comparison')\n        axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=45)\n        \n        # Plot 2: Classifier Comparison\n        sns.boxplot(data=self.results_df, x='Classifier', y='F1_Score', ax=axes[0,1])\n        axes[0,1].set_title('Classifier Comparison')\n        axes[0,1].set_xticklabels(axes[0,1].get_xticklabels(), rotation=45)\n        \n        # Plot 3: Feature Percentage Comparison\n        sns.boxplot(data=self.results_df, x='Feature_Percentage', y='F1_Score', ax=axes[1,0])\n        axes[1,0].set_title('Feature Percentage Comparison')\n        \n        # Plot 4: Metrics Comparison\n        metrics_df = self.results_df.groupby('Classifier')[['F1_Score', 'MCC', 'AUC']].mean()\n        metrics_df.plot(kind='bar', ax=axes[1,1])\n        axes[1,1].set_title('Average Metrics by Classifier')\n        axes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), rotation=45)\n        \n        plt.tight_layout()\n        return fig\n\ndef main():\n    # Create synthetic imbalanced datasets with different imbalance ratios\n    datasets = []\n    \n    # Dataset 1: Moderate imbalance (1:3)\n    X1, y1 = make_classification(n_samples=846, n_features=18, n_classes=2, \n                               weights=[0.75, 0.25], random_state=42)\n    datasets.append(('Dataset 1 (1:3)', pd.DataFrame(X1), y1))\n    \n    # Dataset 2: High imbalance (1:8)\n    X2, y2 = make_classification(n_samples=1484, n_features=8, n_classes=2,\n                               weights=[0.89, 0.11], random_state=42)\n    datasets.append(('Dataset 2 (1:8)', pd.DataFrame(X2), y2))\n    \n    # Dataset 3: Very high imbalance (1:15)\n    X3, y3 = make_classification(n_samples=214, n_features=9, n_classes=2,\n                               weights=[0.94, 0.06], random_state=42)\n    datasets.append(('Dataset 3 (1:15)', pd.DataFrame(X3), y3))\n    \n    analyzer = ImbalancedDataAnalyzer()\n    \n    # Analyze each dataset\n    for dataset_name, X, y in datasets:\n        print(f\"\\n{'='*50}\")\n        print(f\"Analyzing {dataset_name}\")\n        print('='*50)\n        \n        # Set feature names if not present\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n        \n        # Run analysis\n        results = analyzer.analyze_dataset(X, y)\n        \n        # Plot results\n        fig = analyzer.plot_results()\n        plt.savefig(f'{dataset_name.replace(\" \", \"_\")}_results.png')\n        plt.close()\n        \n        # Save results to CSV\n        results.to_csv(f'{dataset_name.replace(\" \", \"_\")}_results.csv', index=False)\n        \n        # Print summary\n        print(\"\\nTop 5 Configurations:\")\n        print(results.nlargest(5, 'F1_Score')[['Feature_Selection', 'Classifier', \n                                              'Feature_Percentage', 'F1_Score', 'MCC', 'AUC']])\n\nif __name__ == \"__main__\":\n    from sklearn.datasets import make_classification\n   def main():\n    # Load datasets\n    datasets = []\n    \n    # Dataset 1\n    df1 = pd.read_csv('../input/your-dataset-folder/dataset1.csv')  # Replace with your path\n    X1 = df1.drop('target_column', axis=1)  # Replace 'target_column' with your target column name\n    y1 = df1['target_column']\n    datasets.append(('Dataset 1', X1, y1))\n    \n    # Dataset 2\n    df2 = pd.read_csv('../input/your-dataset-folder/dataset2.csv')  # Replace with your path\n    X2 = df2.drop('target_column', axis=1)\n    y2 = df2['target_column']\n    datasets.append(('Dataset 2', X2, y2))\n    \n    # Dataset 3\n    df3 = pd.read_csv('../input/your-dataset-folder/dataset3.csv')  # Replace with your path\n    X3 = df3.drop('target_column', axis=1)\n    y3 = df3['target_column']\n    datasets.append(('Dataset 3', X3, y3))\n    \n    analyzer = ImbalancedDataAnalyzer()\n    \n    # Analyze each dataset\n    for dataset_name, X, y in datasets:\n        print(f\"\\n{'='*50}\")\n        print(f\"Analyzing {dataset_name}\")\n        print('='*50)\n        \n        # Run analysis\n        results = analyzer.analyze_dataset(X, y)\n        \n        # Plot results\n        fig = analyzer.plot_results()\n        plt.savefig(f'{dataset_name.replace(\" \", \"_\")}_results.png')\n        plt.close()\n        \n        # Save results to CSV\n        results.to_csv(f'{dataset_name.replace(\" \", \"_\")}_results.csv', index=False)\n        \n        # Print summary\n        print(\"\\nTop 5 Configurations:\")\n        print(results.nlargest(5, 'F1_Score')[['Feature_Selection', 'Classifier', \n                                              'Feature_Percentage', 'F1_Score', 'MCC', 'AUC']])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}